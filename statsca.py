# -*- coding: utf-8 -*-
"""StatsCA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BNHiOpCnF77MZB4WrAS294nx34A8_ApX

___
# **Introduction**
___

[Link to Google Colab](https://colab.research.google.com/drive/1BNHiOpCnF77MZB4WrAS294nx34A8_ApX?usp=sharing)

This report, written by Glenn Lauder and John Barrett, is focused on the interpretation and exploration of a Formula 1 Dataset with 14 CSV files (QPM, 2022). These have each been converted to Pandas DataFrames in order to use many of the Pandas methods for statistical analysis. The data in these DataFrames range from past and present drivers to constructor championship teams, including many of the most prestigious circuits on this planet.

This report aims to achieve and articulate the process of the exploration and understanding needed to learn about the statistics behind the sport, both in content and in methodology.

The **First Section** of this report is the most comprehensive, as we believed that it is crucial to gain a firm understanding of the dataset before approaching more specific variables for later analysis. This section uses a plethora of Python and Pandas methods to get an overview of the 14 DataFrames from a descriptive statistics and variable analysis perspective, which was helpful for dictating our path going into the later sections.

The **Second Section** focuses on correlation analysis, namely between the number of pitstops occuring in each lap. This was an area of interest which was discovered during our work in the First Section, highlighting the importance of the processes in descriptive statistics and variable analysis. Discussion on the conclusions of these outcomes have helped to gain a greater understanding of the meaning and potential of statistical analysis as a whole.

The **Third Section** builds up from the previous work on correlation analysis, working on building a Linear Regression Model that may allow prediction of data in these areas in the future. The accuracy (and inaccuracy) of this model as compared to the actual data can give insight into the potential improvements in methodology that could be implemented going forward.

Reporting on methods and outcomes has been written throughout this work, as to neatly and intuitively tie the graphical and numerical outcomes to their contextual and statistical meanings and interpretations.

___  
# **First Section**: Dataset selection & Descriptive stats
___

## **Part 0**. Imports
"""

import glob
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
import pandas as pd
import numpy as np
import seaborn as sns
import tabulate
from IPython.display import HTML, display
from sklearn.metrics import r2_score

"""___

## **Part 1**. The Dataset & Why We Chose it

  This Formula 1 dataset contains records which date back to the first inception of this racing championship in 1950 and continue all the way to the current season in 2022. We found this dataset to be very interesting, as well as insightful, when it came to understanding the sport and the stats behind it.  

  The reason why we choose F1 is down to the different elements available and the greater selection of qualitative and quantitative variables compared to other datasets. This Dataset is continuously updated from the rich content of records from the F1 official site, which makes for a exciting stats project, as F1 is a data driven sport.

We can upload the dataset CSV files by hosting them on GitHub
"""

githubRepo = [ # These were downloaded in bulk from the dataset on Kaggle, then uploaded to a repo on github for easy access.
    "https://raw.githubusercontent.com/JohnsDadsSon/StatsCA/main/constructor_standings.csv",
    "https://raw.githubusercontent.com/JohnsDadsSon/StatsCA/main/driver_details.csv",
    "https://raw.githubusercontent.com/JohnsDadsSon/StatsCA/main/driver_standings.csv",
    "https://raw.githubusercontent.com/JohnsDadsSon/StatsCA/main/fastest_laps.csv",
    "https://raw.githubusercontent.com/JohnsDadsSon/StatsCA/main/fastestlaps_detailed.csv",
    "https://raw.githubusercontent.com/JohnsDadsSon/StatsCA/main/pitstops.csv",
    "https://raw.githubusercontent.com/JohnsDadsSon/StatsCA/main/practices.csv",
    "https://raw.githubusercontent.com/JohnsDadsSon/StatsCA/main/qualifyings.csv",
    "https://raw.githubusercontent.com/JohnsDadsSon/StatsCA/main/race_details.csv",
    "https://raw.githubusercontent.com/JohnsDadsSon/StatsCA/main/race_summaries.csv",
    "https://raw.githubusercontent.com/JohnsDadsSon/StatsCA/main/sprint_grid.csv",
    "https://raw.githubusercontent.com/JohnsDadsSon/StatsCA/main/sprint_results.csv",
    "https://raw.githubusercontent.com/JohnsDadsSon/StatsCA/main/starting_grids.csv",
    "https://raw.githubusercontent.com/JohnsDadsSon/StatsCA/main/team_details.csv",
]
df_dict = {}  # Empty dict used to store each of our dataframes
for csv in githubRepo:
  filename = csv.split("/")[-1].replace(".csv", "")  # getting just the file's name
  df_dict[filename] = pd.read_csv(csv)  # making a dataframe in our dict with the filename

"""With df_dict[`df`] we now have a dataframe which we can use methods (such as those in pandas) to work with to get a quick understanding of this dataset. Using a dict will help us to iterate over the 14 dataframes for various methods.

___

## **Part 2**. Dataset Info
In this part, we'll explore some relevant info about the F1 DataSet using various methods in python. Along the way, we'll discuss what these methods are, what insight they can give us and give some relevant info about our dataset through using them.

Here we have used the DataFrame.shape function.
"""

tabledata = [["DataFrame", 'Rows', "Columns"]] # Adding in these to label the oncoming table data
for df in df_dict: # Showing the shape (dimensions) of the 14 different files in this dataset with df.shape
  tabledata.append([df,df_dict[df].shape[0], df_dict[df].shape[1]]) # df.tail()
display(HTML(tabulate.tabulate(tabledata, tablefmt='html'))) # using IPython.display and tabulate modules to have a cleaner output

"""It has output the number of all the rows and columns of each DataFrame, from practices to fastestlap details. We can quickly see the structure and make decision on which dataset to explore for further analysis.
E.g. If we want to look at the most columns at once, this may guide us to a DataFrame like `qualifyings`, `practices`, or `pitstops`. We have found a lot of these methods to be very helpful when guiding further analysis.

**Note**: for this output we've used IPython.display and tabulate modules to get a cleaner output with a better readability.

Next, we can get a better understanding of what this data looks like with df.head(), a method which shows the first n rows of a dataframe. By default, the df.head() method gives the first five rows of the dataset.
"""

for df in df_dict: # Showing first 5 rows with df.head()
  print(df + "\n") #Title and new line
  print(df_dict[df].head()) #df.head()
  print("\n") # New Line

"""We can now see what the columns given by the df.shape method are, in a table-like format.

Each DataFrame has a different set of columns with a couple overlapping between DataFrames. We can see that some DataFrames start in different years, such as the constructor champion in 1958 (eight years after the the first F1 race in 1950). We can clearly see by using df.head() that racing structure has evolved through the decades.

df.tail() is like the opposite of the function .head(), showing the last 5 rows by default.
"""

for df in df_dict: # Showing last 5 rows with df.tail()
  print(df + "\n") # Title and new line
  print(df_dict[df].tail()) # df.tail()
  print("\n") # New Line

"""By using .tail() you can see the most recent events in F1 across all DataFrames.

The info() method prints information about the DataFrame.
"""

for df in df_dict: # Print a concise summary of a DataFrame. This method prints information about a DataFrame including the index datatype and columns, non-null values and memory usage.
  print(df + "\n") # Title and new line
  print(df_dict[df].info()) # df.info()
  print("\n") # New Line

"""The information contains the number of columns, column labels, column data types, memory usage, range index, and the number of cells in columns. We have int, floats and objects with count (the count of non-null values) in these DataFrames.

The df.describe() method prints the count, mean, std, value, max, min, as well as quartile ranges for various types of data.
> _Generate descriptive statistics. Descriptive statistics include those that summarize the central tendency, dispersion and shape of a datasetâ€™s distribution, excluding NaN values. Analyzes both numeric and object series, as well as DataFrame column sets of mixed data types._ - [Pandas Docs](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html) (Pandas Dev Team, 2022).
"""

for df in df_dict:
  print(df + "\n") # Title and new line
  print(df_dict[df].describe()) # df.describe()
  print("\n") # New Line

"""The Pandas docs have been immensely useful for understanding the purpose, usage, and application of many different methods. Using this .describe() method in parts 3 and onwards, we will be able to get a much better view of a given variable.

___

## **Parts 3, 4, 5 & 6**. Variable Analysis

In this section we will use relevant statistical methods in order to get a more nuanced understanding of different parts of our dataset. This will consist of statistical analysis on 2 different variables, one qualitative and one quantitative. We have somewhat combined and rearranged these parts in order to fully get a grasp of both variables, though we have made sure to have completed each step in the assignment as is required to achieve ideal learning outcomes.

###**Quantitative Variable**: Average Speed of Fastest Lap
The quantitative variable we've chosen to analyse is the Average Speed of Fastest Laps, which is the `Avg Speed` column in the `fastestlaps_detailed` DataFrame.
"""

avg_speed = df_dict["fastestlaps_detailed"]["Avg Speed"] # avg_speed is the SERIES we will work with

"""Using the aforementioned ".describe" method, we can get a lot of insight into the behaviour of this variable. This time we will be using the method on a **series**, not a DataFrame."""

avg_speed.describe()

"""From this output, we get quite a few interesting bits of information.

The **count** of non-null values is 9400, a very high number of entries, though it doesn't match the full number of rows in the dataset. This will be discussed more when we approach the end of this section.


The **mean** fastest speed is 202.5km/h, which makes sense for F1, putting into perspective just how fast these events are.

The **minimum** fastest lap here is recorded as 0km/h, which we can assume is someone not completing a lap, or being discounted in some way.

The **maximum** fastest lap in our DataFrame is 257.3km/h. To clarify, this isn't their top speed, it's their mean speed across the duration of their fastest lap.

The **standard deviation** in fastest laps is 21.6km/h.

The **interquartile range** can be calculated with this method too, as we are given values for 25% and 75%. Performing this calculation, we get an interquartile range of 22.3. This is the middle 50% of our data, meaning that half of all fastest laps occur within a narrow speed range of 22.3km/h.

The **median** fastest lap speed is 203.6km/h, slightly faster than the median.

Using this final bit of insight, we can start to look further into the distribution of this variable.

When the median is greater than the mean, that can often indicate that there's a negative (or left) skew in the data. When having a visual tool to inspect the data from a different perspective, we can achieve a more solidified understanding.
"""

avg_speed_hist = avg_speed.plot(kind="hist", bins=60, range=(0,300), figsize=(8,6), title="Frequency of average fastest lap speeds").set_xlabel("Average speed of fastest lap (km/h)")

"""A histogram provides an intuitive visual way to see where values lie in a column. This histogram plot uses a range of 300km/h (a little above our maximum value) and 60 bins, grouping the data into ranges of 5km/h. With this visualisation, we can see that there are a number of outliers on the left of the plot, with far fewer fast outliers on the right of the plot. This further suggests that the skew of this variable is a negative (or left) skew.

We can further explore this with a **box plot**, which is another visual tool that excels in diplaying the **interquartile range** and **skewness** of a variable.
"""

avg_speed_boxplot = avg_speed.plot.box(figsize=(8, 6), title="Average speed of fastest laps boxplot", ylabel="Average speed of fastest lap (km/h)")

"""This, once again, shows clear signs of a negative skewness. The interquartile range (shown by the box) is very narrow, spanning that 22.3km/h range which was calculated with the .describe() method. There are a number of slower outliers on the negative side, with very few faster laps on the positive side. This makes sense considering the sport. They drive so fast that greater speeds are very rare or not possible, while it always a possibility that there will be slower laps throughout an event.

In Python, there a lot of methods that can give us the answer to our question, though they often lack the explanations and insight that hands-on exploration of a dataset can give us. One of these tools is the Series.skew() method, which returns the skewness of a variable.
"""

avg_speed.skew()

"""This output of -0.83 confirms our ideas of negative skewness from this variable, though it was much more valuable from a learning and understanding perspective to analyse the variable from multiple perspectives, with a variety of tools, both numerical and visual.

Even with a dataset as complex and full as this F1 dataset, there can often be missing values from variables, so it is important to understand when and why that might be the case.

Series.isnull() is a method which returns either True or False for each row, True if there is no value for that row, False if there is a value.
"""

avg_speed.isnull()

"""We can see that some of the early rows have null values, implying that they may have been included at a later date.

We can use the sum() method to add up the true values of this new series.
"""

avg_speed.isnull().sum()

"""This makes sense, as the .shape() method for `fastestlaps_detailed` returned 15512 rows, and the .describe() method for `Avg Speed` returned a 9400 non-null count. These 6112 null entries account for that discrepancy.

By exploring our dataset with different methods earlier, we found that the minimum value in the series was 0km/h. This can raise some questions, as it should be impossible to complete a lap with a mean speed of 0km/h. A likely reason for this being the case is that some drivers who hadn't completed a fastest lap had an average speed of 0km/h inputted instead of `Null`. Another possibility is that due to a technicality in the ruleset, laps could be completed without the timer having an effect on the final result, leading to a drivers fastest lap being one that didn't have a time to calculate average speed with.

There is always more analysis to be done and more questions to ask about any variable, though for the sake of this report we believe that this section has adequately covered different attributes of the `Avg Speed` variable.

### **Qualitative Variable**: Nationality
The qualitative variable we've chosen to analyse is the nationality of drivers, which is the `Nationality` column in the `driver_standings` DataFrame. In this section, the words country, nation, and nationality are used overlappingly, as to avoid confusion. Not all entries here are to be considered nations, Notably the Russian Automobile Federation, but for the sake of consistency these grey areas have not been marked differently.

Country codes (and more information) for automotive sports can be found [here](http://www.motorsportmemorial.org/misc/colours.php?db=ct) (The Motorsport Memorial Team, 2022).
"""

nation = df_dict["driver_standings"]["Nationality"]

"""As this is qualitative data, we don't have as much numerical statistical methods to use to understand this variable. However, we can still look to get a solid understanding of the dispersion and traits of the `Nationality` variable."""

nation.describe()

"""With this familiar describe() method, we can see that out of the 1618 standings entries, all of these drivers came from 37 different countries. The most frequent in the series is Great Britain (GBR) with nearly 300 appearances, making up 18% of the total standings.

A full series of the **value counts** can be found in the cell below.
"""

nation.value_counts()

"""A further look into the distribution of this variable can be found with the describe method, which gives us the following output:"""

nation.value_counts().describe()

"""The **count** of non-null unique nations is 37.

The **mean** number of drivers per nation is 43.7, which is significantly higher than the **median** of 24 which is the nation of The Netherlands.

The **minimum** is 1, shared by the Russian Automobile Federation (RAF), China (CHN), Indonesia (INA) and Malaysia (MAS).

The **maximum**, as stated before, is Great Britain (GBR).
"""

nations_bar = nation.value_counts().plot(kind="bar", title="Drivers per Nationality",figsize=(16,6), width=.8)
nations_bar.set_ylabel("Total Drivers")
nations_bar = nations_bar.set_xlabel("Nationality Codes")

"""Nationaility of participating drivers in this graph show a higher number of European and Central/South American drivers in F1, followed by numerous other countries. It is important to note that this graph and its associated series do not account for the population of countries, nor do they reflect the performance of the drivers.

The plot shows that the majority of drivers come from only 5 countries, those being Great Britain (GBR), Italy (ITA), France (FRA), Germany (GER) and The United States of America (USA).
"""

top_5 = nation.value_counts()["GBR"] + nation.value_counts()["ITA"] + nation.value_counts()["FRA"] + nation.value_counts()["GER"] + nation.value_counts()["USA"] # Those 5 countries added up
print(top_5)
# We can actually do this with nation.value_counts()[0:5].sum() too.

"""If we divide that by the total amount of drivers"""

str((round((top_5/nation.value_counts().sum()*100), 2))) + "%" # Rounding to 2 decimal places and converting to percentage

"""We can see that these 5 countries make up over 57% of all recorded drivers' nationalities.

We wanted to make sure that there weren't any missing values, which could mean drivers' nations not being recognised. As was the case for the quantitative data, we can do this with the Series.isnull() method, which we sum up the `True` values (True is returned from .isnull() when the value is null).
"""

nation.isnull().sum()

"""Thankfully this DataFrame has no missing values for the nationality of drivers, featuring a diverse range of nations.

Although there are fewer numerical insights to be gained from using descriptive statistic techniques on qualitative variables such as nationality, it is still a highly useful area of statistics for understanding the data we're working with, as well as providing the methods and vocabulary for navigating datasets such as this Formula 1 dataset.

___
# **Second Section**: Correlation Analysis
___

In this correlation analysis section we will analyse two variables from our dataset. Correlation analysis can be performed to see the connection between two variables, where the correlation can be postive, negative or no correlation at all. The method we can use to define the correlation is the Spearman's rank correlation coefficient. The Spearman's rank is based on the ranked values for each variable rather than the raw data.

Another method that can be applied is the  Pearson Product-Moment Coefficient, a widely used measure in statistial analysis which is often used to determine the strength of the relationship between two variables in scientific research studies. It is important to note that the Pearson correlation coefficient only measure linear relationships and may not be suitable for measuring non-linear relationpships.

The variables that we will observe are both within the `pitstops` DataFrame. These are `Lap` and `Stops`. We will not be performing correlation analysis on these directly, instead we are going to investigate the relationship between laps in an F1 event and the total amount of pitstops.

The `pitstops` DataFrame has several interesting variables:
"""

pitstops = df_dict["pitstops"]
pitstops.head()

"""Originally, we had looked into correlations of total pitstop time and finishing position, but ran into numerous errors, as total pitstop time is greatly effected by occurences during an event, which would make correlation analysis very unclear, as well as introducing issues in Section 3.

It is important to check some of the information about the DataFrame before using methods designed for specific dtypes. The DataFrame info will be found using the df.info() method.
"""

pitstops.info()

"""Every column is either an object or of type int64, so we should be able to work without unexpected type errors.

We'll group these rows by the `Lap` variable and count them using the .count() method, as we're aiming to calculate stops per lap.
"""

laps_group = pitstops.groupby(["Lap"]).count()["Stops"]
laps_group = laps_group.reset_index() # Laps is now a column, making other methods easier
laps_group.head()

"""It is now showing the count of different pitstops that occured in the same lap number (not the same race, instead it's the total number of pitstops that have occured in a given lap across ALL events in the dataset). We can visualize this with a plot."""

laps_plot = laps_group.plot(x="Lap", y="Stops", title="Total stops per lap", figsize=(8,6)).set_ylabel("Total Stops")

"""From here, we had originally planned to do the cumulative sum of stops by each lap, which by definition must be equal or increase from lap to lap. We found this to not be very interesting or insightful to analyse, as the correlation seemed self-evident. We'll instead be continuing with the total stops per lap, across the entire DataFrame.

Visual inspection of this plot gives some insight into the times where drivers have to pitstop. There is a high number in the first couple of laps, followed by a quick drop which picks up in the double digits. This follows a steady decline until the end of the laps.

Lastly, we can use the pandas method df.corr() to get a correlation coefficient. This takes an input of which correlation coefficient to calculate, that being 'pearson' or 'spearman' in our case
"""

pears = laps_group.corr('pearson')
spear = laps_group.corr('spearman')

pears

spear

"""We can observe that both Pearson and Spearman methods give us a **correlation of -0.71**. This equates to a strong negative correlation for Laps and Pitstops.

Although this is a strong correlation. it is important to ask questions about causality. We cannot say for certain if lower laps cause more pitstops, or if higher laps cause fewer pitstops.

We believe that a likely explanation is that at at the beginning of Formula 1 events, where many drivers are still in the race, there are going to be more errors and issues that require a pitstop to mitigate. Nearing the end of events, where the fewest drivers are still in the running for a good position, they are less concerned about longevity and would be more focused on maintaining position. As with many avenues of analysis, more investigation may need to be done in order to further understand this data.

___
# **Third Section**: Linear Regression
___

Using the same variables in section 2, we will be building a linear regression model that allow use to predict information about those variables. This allows us to predict a continuous outcome variable based on one or more independent variables. It's based on the assumption that the relationship between the two variables will linear. `LinearRegression` is a class from the `sklearn.linear_model` which we will use to compare with the `polyfit` method from `NumPy`

More types of regression models include ridge regression, lasso regression and elastic net regression. These models are similar to linear regression, but they include additional regularization terms to prevent overfitting.

For this analysis, we want to know to what degree the total pitstop count can be predicted by the lap number.

A great first step when approaching any kind of analysis is to plot the data. The `laps_group` DataFrame from the Second Section will be used throughout this section aswell.
"""

scatter_laps = sns.scatterplot(data=laps_group, x="Lap", y="Stops")
scatter_laps = scatter_laps.set_title("Scatterplot of total stops per lap")

"""This plot matches our plot from the Second Section, though it is good to always have a visual reference.

## Regression line model with Numpy

This linear regression model will use the `NumPy` library, specifically the `polyfit` method.
"""

pits_fit = np.polyfit(laps_group.Lap, laps_group.Stops, 1)
pits_fit # Returns an array

"""The output gives us a = -6.03 and b = 500.14. With these two values we can now present our Lineal Regression Model:
* Laps: L
* Stops: S

Linear Regression Model: **S = 500.14 - 6L**

By taking the original scatter plot and imposing our Linear Regression Model, we can see how the behaviour of pitstops somewhat follows a linear descent.
"""

reg_line = laps_group.plot(kind='scatter', x='Lap', y='Stops', color='blue', alpha=0.5, figsize=(8,6)) # Original scatterplot
plt.plot(laps_group.Lap, pits_fit[0] * laps_group.Lap + pits_fit[1], color='red') # Plotting regression line
plt.text(10, 25,'y={:.3f}{:.3f}x'.format(pits_fit[1],  pits_fit[0]), color='red') # Line equation to 2 decimal points
plt.legend(labels=['Pitstops Regresion Line', 'Count of Stops', 'Stops'])
plt.title('Relationship between Laps and Stops')
plt.xlabel('Lap')
plt.ylabel('Total Stops');

"""## Regression line model with SciKit Learn

Another option for creating a linear regression model is the `LinearRegression` class from the `sklearn.linear_model` module.
"""

lr_pits = LinearRegression() # Creating a linear regression object
lr_pits.fit(laps_group[['Lap']].values, laps_group['Stops']) # fitting linear regression
print("Intercept: ", lr_pits.intercept_) # getting the intercept of the line
print("Coefficient: ", float(lr_pits.coef_)) # get the slope coefficient

"""This gives us the same output as our `NumPy` method.

## Get a predicted value with Numpy and Scikit Learn

For `NumPy` and `Scikit`, we can use `polyval` and `predict` respectively to predict values using our linear regression models. As they both use the same coefficients, they should produce the same output.
"""

print(np.polyval(pits_fit, [10])) # predictions using numpy
print(lr_pits.predict([[10]])) # predictions using scikit learn

"""## Coefficient of Determination

With the previous methods of prediction using our linear regression models, we can calculate the **coefficient of determination**. This coefficient indicates how well our models can predict the actual values of our variables. We will use `r2_score` from the `sklearn.metrics` module to calculate the total sum of squares.
"""

actual_stops = laps_group.Stops.values # Actual pitstop values
laps_counts = laps_group.Lap.values # Lap counts, for iteration
predicted_stops = [] # Empty array which will have predictions appended to
for lap in laps_counts:
  predicted_stops.append(float((np.polyval(pits_fit, [lap])))) # NumPy Method
R_square = r2_score(actual_stops,predicted_stops)
print('Coefficient of Determination:', R_square)

"""With this calculation we can estimate that 50.5% of the total sum of squares can be explained by using the estimated regression equation to predict the total pitstops count, where the remainder is error.

___
# **Conclusion**
___

Throughout the body of this report, this dataset was explored and analysed in 3 different sections. The **First Section** aimed to gain a general understanding of the dataset as a whole through descriptive statistics and variable analysis. Parts 3 to 6 are notable for the analysis of 2 specific variables, Average Speed and Nationality. These are quantitative and qualitative variables respectively. Parts 1 and 2 were focused on generating and exploring DataFrames from the dataset. We then would view the shape, size, and purpose of each DataFrame using a variety of Python and Pandas methods, as these were Pandas DataFrames.

The **Second Section** involved diving deeper into specifics of relationships between variables with **Correlation Analysis**. Using the understanding gained from Section 1, the variables chosen to analyse were the pitstops per lap. We had an interest on how this may increase or decrease over the couse of a race, and what each outcome could mean. A strong **correlation of -0.71** was calculated between these variables, meaning that as the lap number increases, we expect to see a general decrease in the number of pitstops. We cannot confirm that this implies causality from one variable to another, though we suspect that, as stated in the Second Section, a likely explanation is that at at the beginning of Formula 1 events, where many drivers are still in the race, there are going to be more errors and issues that require a pitstop to mitigate. Nearing the end of events, where the fewest drivers are still in the running for a good position, they are less concerned about longevity and would be more focused on maintaining position. We would be enthusiastic to explore this relationship with further statistical analysis and further external research.

The **Third Section** continued work with correlation analysis in the previous section in order to build a **Linear Regression Model** that may allow us to model/predict behaviour of these variables. Our correlation between variables wasn't strictly linear, though we approached this section with the aim to have an equation to model the correlation as closely as possible. Using methods from the NumPy library, we were able to generate the following equation:

> Laps: L
>
> Stops: S
>
>Linear Regression Model: **S = 500.14 - 6L**

When we plotted this equation onto our data, there was a close (but not exact) match. We would go on to calculate the **coefficient of determination** in order to quantify how closely our model matches the data. This coefficient was calculated to be **0.5046**, meaning that we can estimate that 50.5% of the total sum of squares can be explained by using our Linear Regression Model to predict corresponding values. Although not perfect, we were satisfied with this outcome and the learning outcomes that we achieved by performing these methods in all three sections.

___
# **References**
___

> Pandas Dev Team, 2022. pandas documentation â€” pandas 1.5.2 documentation [Online]
>
> Available at: https://pandas.pydata.org/docs/index.html
>
> [Accessed 07 12 2022].

> QPM, 2022. Kaggle.com. [Online]
>
> Available at: https://www.kaggle.com/datasets/debashish311601/formula-1-official-data-19502022
>
>[Accessed 08 12 2002].

> The Motorsport Memorial Team, 2022. Motorsport Memorial - Acknowledgements. [Online]
>
>Available at: http://www.motorsportmemorial.org/misc/colours.php?db=ct
>
> [Accessed 10 12 2022].
"""